{
  "agent_id": "coder2",
  "task_id": "task_7",
  "files": [
    {
      "filename": "tests/test_stackelberg.py",
      "purpose": "Unit tests for Stackelberg equilibrium computation and edge cases",
      "priority": "medium",
      "dependencies": [
        "pytest",
        "numpy"
      ],
      "key_functions": [
        "test_equilibrium_computation",
        "test_case_transitions",
        "test_parameter_boundaries"
      ],
      "estimated_lines": 200,
      "complexity": "medium"
    },
    {
      "filename": "tests/test_exp3p.py",
      "purpose": "Unit tests for EXP3.P algorithm implementation and regret bounds",
      "priority": "medium",
      "dependencies": [
        "pytest",
        "numpy"
      ],
      "key_functions": [
        "test_regret_bounds",
        "test_action_selection",
        "test_weight_updates"
      ],
      "estimated_lines": 150,
      "complexity": "medium"
    }
  ],
  "project_info": {
    "project_name": "SoftInducementSteeringFramework",
    "project_type": "optimization",
    "description": "Implementation of a soft inducement framework for steering no-regret players toward desired action profiles in mediator-augmented two-player Bayesian normal-form games. The system combines information design (signaling policies) with incentive mechanisms (payments) to achieve steering objectives while minimizing mediator costs. It features a Stackelberg game formulation where the mediator acts as leader committing to signaling policies before players optimize their strategies, followed by repeated game execution with EXP3.P no-regret learning algorithms.",
    "key_algorithms": [
      "StackelbergEquilibriumComputation",
      "EXP3PNoRegretLearning",
      "BayesianCorrelatedEquilibrium",
      "InformationDesignOptimization",
      "IncentiveMechanismDesign"
    ],
    "main_libraries": [
      "numpy",
      "scipy",
      "matplotlib",
      "pandas",
      "cvxpy",
      "torch",
      "tqdm",
      "seaborn",
      "gym",
      "tensorboard"
    ]
  },
  "paper_content": "PDF: cs.SY_2508.21672v1_A-Soft-Inducement-Framework-for-Incentive-Aided-St.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nA Soft Inducement Framework for Incentive-Aided Steering of\nNo-Regret Players\nAsrin Efe Yorulmaz, Raj Kiriti Velicheti, Melih Bastopcu, and Tamer Bas \u00b8ar\nAbstract \u2014 In this work, we investigate a steering problem in a\nmediator-augmented two-player normal-form game, where the\nmediator aims to guide players toward a specific action profile\nthrough information and incentive design. We first characterize\nthe games for which successful steering is possible. Moreover,\nwe establish that steering players to any desired action profile is\nnot always achievable with information design alone, nor when\naccompanied with sublinear payment schemes. Consequently,\nwe derive a lower bound on the constant payments required per\nround to achieve this goal. To address these limitations incurred\nwith information design, we introduce an augmented approach\nthat involves a one-shot information design phase before the\nstart of the repeated game, transforming the prior interaction\ninto a Stackelberg game. Finally, we theoretically demonstrate\nthat this approach improves the convergence rate of players\u2019\naction profiles to the target point by a constant factor with high\nprobability, and support it with empirical results.\nI. I NTRODUCTION\nIn strategic interactions, information plays a crucial role\nin shaping the decisions of agents. A mediator can influence\nthe outcome of a game by controlling the information\navailable to the players [1]. This control is particularly\nvaluable in multi-agent settings, where individual decisions\nhave system-wide effects [2]. We motivate the integration of\ninformation design into a steering problem by highlighting\nits applications across economics, governance, and digital\nmarkets. In economic contexts, persuasion mechanisms are\nwidely employed to influence consumer behavior alongside\nmonetary incentives [3]. Governments use public information\ncampaigns to encourage participation in social programs [4],\nadvertisers strategically release information to shape con-\nsumer preferences [5], and online platforms curate content to\ninfluence user engagement [6]. Similarly, financial markets\nand regulatory bodies use signaling mechanisms to steer\ninvestor expectations [7], affecting market conditions [8].\nWhile long-term (asymptotic) effects are often the focus\nof traditional analysis, many real-world scenarios demand a\nstronger emphasis on transients. For instance, in emergency\nsituations such as disaster management [9] or financial crises\n[10], ensuring short-term adaptation is crucial for mitigating\nimmediate risks. These considerations motivate our results on\nResearch of AEY , RKV , and TB was supported in part by the US Army\nResearch Office (ARO) Grant W911NF-24-1-0085 and in part by the NSF\nGrant ECCS 23-49418. Research of MB was supported in part by Tubitak\nBilgem EDGE-4-IoT and Tubitak 2232-B Fellowship (Project No:124C533).\nAsrin Efe Yorulmaz, Raj Kiriti Velicheti, and Tamer Bas \u00b8ar are with\nthe Coordinated Science Laboratory at the University of Illinois Urbana-\nChampaign, Urbana, IL, USA-61801. Melih Bastopcu is with the Depart-\nment of Electrical and Electronics Engineering, Bilkent University, Ankara,\nTurkey, 06800. (Emails: {ay20,rkv4,basar1 }@illinois.edu ,\nbastopcu@bilkent.edu.tr )improving transient regret, ensuring that strategic decisions\nalign with objectives in both the short run and the long run.\nOur work focuses on an information-aided incentive\ndesign problem in a two-player normal-form \u201cinvestment\ngame\u201d, which is repeated over Trounds. Also, due to the\nnature of the information design problem, the games we con-\nsider are inherently Bayesian Normal Form Games (BNFGs)\n[11]. The players, modeled as no-regret learners employing\nEXP3.P algorithms [12], receive public signals about the\nstate of the world from the mediator and accordingly choose\nactions, as the state of the world is not observable to them.\nThe mediator\u2019s objective is to steer the players toward a\nspecific strategy profile in an empirical sense. However, we\ndemonstrate that achieving this objective using information\ndesign alone is not always feasible. Furthermore, we show\nthat this goal cannot be accomplished with sublinear pay-\nments for all cases. The results on the feasibility of successful\nsteering are provided in Table I.\nTo address these limitations we propose an augmented\nmethod that incorporates a round of information designs prior\nto the repeated game, which can be modeled as a Stackelberg\ngame [13]. This provides better initial conditions for the\nplayers, thereby improving the convergence rate of their\ndirectness gap to \u02dcO\u0010\u221a\nK\n\u03ba\u221a\nT\u0010\n4p\nln(1/\u03c0\u2217) + 2p\nln(K/\u03b4)\u0011\u0011\nfor each signal instance, where \u03bais the minimum deviation\ncost from the best hindsight action, and \u03c0\u2217is the proba-\nbility of choosing the best hindsight action, compared to\nthe usual \u02dcO\u0010\u221a\nK\n\u03ba\u221a\nT\u0010\n4p\nln(K) + 2p\nln(K/\u03b4)\u0011\u0011\nrate, which\nholds with probability of at least 1\u2212\u03b4, where Krepresents\nthe number of available actions and Tdenotes the number\nof time steps the game has progressed [12].1Thus, the\nimprovement for each signal instance improves the overall\nconvergence bound.\nOur work aligns with research on steering rational agents\nthrough incentive design [14], [15]. The closest work to\nour setting is [16], which incorporates both incentive and\ninformation design. However, their focus is on how to guide\nno-regret agents toward a Nash equilibrium and its variants,\nwhereas our goal is a more general notion of action profiles.\nAdditionally, we optimize for mediator\u2019s signaling strategy,\nwhich we demonstrate to be non-trivial. Table I summarizes\nour main contributions and contrasts them with key results\nfrom a closely related work of [16].2\nOur work also relates to the strategic communication\n1We use \u02dcO(\u00b7)to denote the leading term in Tand its constants.\n2In Table I, a \u2713denotes that the given action profile can be reached by\nthe corresponding method, and a \u00d7indicates existence of a counterexample.arXiv:2508.21672v1  [cs.GT]  29 Aug 2025\n\n--- Page 2 ---\nproblem, originating from [17] and formalized as Bayesian\npersuasion in [18]. Furthermore, we leverage extensions of\ncorrelated equilibrium to Bayesian games from [19]. Lastly,\nour approach relates to multi-armed bandits in adversarial\nenvironments, as repeated normal-form games can be mod-\neled in this framework. Thus, we leverage results from [20]\nto analyze no-regret learning algorithms in Bayesian games.\nTABLE I\nCOMPARISON OF GAME TYPES AND PROPERTIES\nPropertyBNFG with\nStrictly\nDominating\nStrategiesPerfect\nInformation\nNormal Form\nGamesBNFG w/o\nStrictly\nDominating\nStrategiesBNFG w/o\nStrictly\nDominating\nStrategies\nTargeted\nPointsDominant\nStrategy Eq.Pure Nash\nEquilibriumBayes CEMediator\nDecided Pt.\nInformation\n(Advice)\u2713(Lem. 2) - \u00d7(Thm. 2) \u00d7(Thm. 2)\nSublinear\nPayments\u2713(Lem. 2) \u2713[16] \u00d7[16] \u00d7(Thm. 3)\nSublinear\nPayments\n& Advice\u2713(Lem. 2) - \u2713[16] \u00d7(Thm. 3)\nLinear\nPayments\u2713(Lem. 2) \u2713[16] \u2713[16] \u2713[16]\nII. P ROBLEM FORMULATION\nWe consider a steering problem involving a mediator in\na game with two players, each making investment decisions\nunder uncertainty. This uncertainty is captured by the state of\nthe world denoted by \u03b8\u2208\u0398 ={G, B}, which is not directly\nobservable by the players but is known and leveraged by the\nmediator. The prior probability of the good state ( G) is given\nby\u03c8(G) =\u03c8, and of the bad state ( B) with \u03c8(B) = 1\u2212\u03c8,\nwhere 0< \u03c8 < 1, and is known to all players.\nEach player i\u2208 I ={1,2}chooses an action ai\u2208\nA={I, N}, where Idenotes invest andNdenotes not\ninvest . Furthermore, we denote the other player\u2019s actions\nbya\u2212i\u2208 A . Each player aims to maximize its received\nutility in each repetition of the game by trying to leverage\nthe information provided by the mediator. To capture inter-\nplayer externalities, we introduce feature vectors f1,f2\u2208Rd,\nrepresenting characteristics of players 1 and 2, respectively.\nThe externality parameter zis modeled as a function of the\nalignment between these feature vectors, z=\u03d5(\u27e8f1,f2\u27e9),\nwhere \u27e8\u00b7,\u00b7\u27e9denotes the inner product. The function \u03d5:\nR\u2192R+is monotonically increasing, reflecting the intuition\nthat greater alignment between the players\u2019 features leads\nto stronger externalities. Furthermore, we assume symmetry\namong the players; thus, players share identical preferences\nover outcomes and are equally affected by the externality\nparameter z.\nIn contrast, the mediator aims to steer players into the\npoint of her desire. In our framework, the mediator, who\nknows the game structure and players being no-regret learn-\ners, provides monetary incentives along with public signaling\nregarding the state of the world. We utilize the standard\ndefinition of regret formalized as follows.Definition 1: LetAbe the set of possible actions available\nto a player. Suppose that the player selects an action xt\u2208 A\nat each time step t\u2208 {1,\u00b7\u00b7\u00b7, T}and receives a correspond-\ning reward ut(xt). Then, the external regret is defined as:\nR(T) = max\nx\u2217\u2208ATX\nt=1ut(x\u2217)\u2212TX\nt=1ut(xt). (1)\nFormally, the mediator first commits to a stationary sig-\nnaling policy \u03c0(s|\u03b8), which specifies the probability of\nsending signal s\u2208 {g, b}=Sgiven the underlying state\n\u03b8. The signaling strategy is parameterized as, \u03c0(g|G) =\n1\u2212\u03c0(b|G) = \u03b1, \u03c0(g|B) = 1 \u2212\u03c0(b|B) = \u03b2,where\n0\u2264\u03b1, \u03b2\u22641. Furthermore, we assume these parameters\nstays constant throughout the repeated game. Additionally,\nthe mediator can select a payment function \u03bdi:A \u00d7 A \u2192\n[0, P]for each player i, where each \u03bdiis continuous in the\nplayer\u2019s action. The modified utility for player ibecomes\nv(t)\ni(at\ni, at\n\u2212i, \u03b8t) =ui(at\ni, at\n\u2212i, \u03b8t)+\u03bdi(at\ni, at\n\u2212i).Furthermore,\nwe introduce the notion of the directness gap, which can be\ndefined over the deviations between the joint actions of the\nplayers, x(t), and the target profile of the mediator d\u2208 A\u00d7A\nthroughout the game, as:\n\u03b4(T) =1\nTTX\nt=11{x(t)\u0338=d}. (2)\nWe now introduce the steering problem. In general, the\nsteering problem asks whether and under what conditions\nplayers can be guided to a specific action profile. The\nmediator\u2019s objectives are twofold. First, the time-averaged\npayments must be minimized. Second, the players\u2019 actions\nshould become indistinguishable from the target equilibrium\nd\u2208 A \u00d7 A , meaning that the directness gap converges to 0.\nThus, we investigate how information design can be incorpo-\nrated to improve the steering capabilities of the mediator in\ngames that involve information asymmetry. To denote how\nthe repeated game proceeds, we define the decision rule\n\u03c3i,t(ai|s)as a time-dependent variable that stands for player\nichoosing action aigiven the signal sat time step t. Then,\ndenoting the history of the actions until the timestep tbyht,\nthe game proceeds at each time-step as follows:\n1) Mediator commits to a stationary signaling policy,\n\u03c0(\u00b7|\u03b8), and a stationary incentive mechanism, \u03bdi.\n2) At time t, the state, \u03b8t, is realized.\n3) Mediator samples a public signal st\u223c\u03c0(\u00b7|\u03b8t)from\nthe signaling policy \u03c0.\n4) The players observe the signal st, and each player i\nsamples its action at\ni\u223c\u03c3i,t(\u00b7|st, ht).\n5) Each player receives a reward based on its actions and\nthe realized state, v(t)\ni=ui(at\ni, at\n\u2212i, \u03b8t)+\u03bdi(at\ni, at\n\u2212i).\n6) Players update their strategies, \u03c3i,t(\u00b7|st+1,ht+1), ac-\ncording to v(t)\ni. Steps 2\u20136 are repeated until t=T.\nAs, the mediator chooses its policy based on the underly-\ning static game, we introduce a suitable notion of equilibrium\nand solution set. The payoff matrix of the focused game is\n\n--- Page 3 ---\ngiven by:\nPlayer 2: I Player 2: N\nPlayer 1: I(z+y\u03b8, z+y\u03b8) ( z,0)\nPlayer 1: N (0, z) (0 ,0)\nHere, y\u03b8denotes either yGoryBbased on the realized\nstate \u03b8\u2208 {G, B};yGandyBare parameters representing\nthe additional payoffs in good and bad states, respectively,\nwithyB<0< yG. Then, the stationary joint action matrix\nformed according to a provided signal sjis given as:\nPlayer 2: I Player 2: N\nPlayer 1: I \u03b3j \u03b1j\u2212\u03b3j\nPlayer 1: N \u03b1j\u2212\u03b3j 1\u22122\u03b1j+\u03b3j\nHere, \u03b1jis the probability of a player playing action I\ngiven signal sjand\u03b3jis the probability that both players\nplay action Igiven signal sj. These probabilities satisfy 0\u2264\n\u03b3j\u2264\u03b1j\u22641and1\u22122\u03b1j+\u03b3j\u22650. Letting \u03c3(a|s),\nor\u03c3(ai, a\u2212i|s)denote the stationary probability of joint\naction agiven signal s, the expected utility for player iis\nthen defined as:\nE[ui] =X\n\u03b8\u03c8(\u03b8)X\ns\u03c0(s|\u03b8)X\naiX\na\u2212i\u03c3(ai, a\u2212i|s)ui(ai, a\u2212i, \u03b8)\n=\u03c8\u03c0(g|G) [\u03b3g(z+yG) + (\u03b1g\u2212\u03b3g)z]\n+\u03c8\u03c0(b|G) [\u03b3b(z+yG) + (\u03b1b\u2212\u03b3b)z]\n+ (1\u2212\u03c8)\u03c0(g|B) [\u03b3g(z+yB) + (\u03b1g\u2212\u03b3g)z]\n+ (1\u2212\u03c8)\u03c0(b|B) [\u03b3b(z+yB) + (\u03b1b\u2212\u03b3b)z].(3)\nThus, substituting the signaling probabilities, \u03c0(g|G) =\u03b1\nand\u03c0(g|B) =\u03b2, we obtain:\nE[ui] =\u03c8\u03b1(\u03b3gyG+\u03b1gz) +\u03c8(1\u2212\u03b1)(\u03b3byG+\u03b1bz) (4)\n+(1\u2212\u03c8)\u03b2(\u03b3gyB+\u03b1gz)+(1\u2212\u03c8)(1\u2212\u03b2)(\u03b3byB+\u03b1bz).\nFollowing this, a natural notion of equilibrium in games\nwith publicly observed signals is Bayes Correlated Equilib-\nrium (BCE) [19], defined formally as follows.\nDefinition 2: A strategy profile \u03c3(a|s)is said to belong\nto the BCE set if no player has an incentive to deviate for\nany signal sand any alternative action a\u2032\ni\u2208 {I, N}. This\ncondition, known as the BCE compliance, is given by:X\n\u03b8X\na\u2212i\u03c8(\u03b8)\u03c0(s|\u03b8)\u0000\n\u03c3(ai, a\u2212i|s)ui(ai, a\u2212i, \u03b8)\n\u2212\u03c3(a\u2032\ni, a\u2212i|s)ui(a\u2032\ni, a\u2212i, \u03b8)\u0001\n\u22650.(5)\nAdditionally, the variables must satisfy the following fea-\nsibility conditions as mentioned above:\n0\u2264\u03b3j\u2264\u03b1j\u22641,1\u22122\u03b1j+\u03b3j\u22650,\u2200j\u2208 {g, b}.(6)\nWe next address steering no-regret players toward a target\nstrategy profile.\nIII. S TEERING NO-REGRET PLAYERS TOWARD THE\nTARGET STRATEGY POINT\nThe problem of steering no-regret players toward a specific\nstrategy point in our setting can be reduced, without loss of\ngenerality, to the problem of guiding players to (I, I). This is\nbecause the role of (I, I)ranges from being strictly dominant\nto being dominated as we vary the utilities. To achieve this,\nwe formalize the definitions of no-regret learning dynamics\nand Bayes-CCE (BCCE) set, and provide a proof of con-\nvergence of no-regret players to such an equilibrium. Thisapproach builds upon analogous results presented in [20],\nwhere the proof was provided based on population-based\ninterpretation of Bayesian games. Formally, we define the\nno-regret property as follows:\nDefinition 3: An algorithm is said to be a no-regret\nalgorithm if its external regret, as defined in (1), grows\nsublinearly with respect to the number of time steps T.\nFormally, we have:\nR(T)\nT=1\nT \nmax\nx\u2217\u2208ATX\nt=1ut(x\u2217)\u2212TX\nt=1ut(xt)!\n\u21920asT\u2192\u221e.(7)\nFurthermore, we define a BCCE set as follows.\nDefinition 4: A strategy profile \u03c3(a|s)is said to belong\nto the BCCE set if, for each player i, and any alternative\naction a\u2032\ni\u2208 Ai, the following condition holds:\nE\u03c8,\u03c0,\u03c3h\nui(ai, a\u2212i, \u03b8)i\n\u2265E\u03c8,\u03c0,\u03c3h\nui(a\u2032\ni,a\u2212i, \u03b8)i\n.(8)\nThen, for the convergence of no-regret players in repeated\nBNFGs, we present the following lemma.\nLemma 1: Under separate no-regret learning dynamics for\neach signal instance, the joint empirical distribution\nDT(\u03b8, s, a 1, a2)=1\nTTX\nt=11{\u03b8t=\u03b8, st=s, at\n1=a1, at\n2=a2}(9)\nconverges almost surely to D(\u03b8, s, a 1, a2) =\u03c8(\u03b8)\u03c0(s|\u03b8)\n\u03c3(a1, a2|s). Moreover, D(\u03b8, s, a 1, a2)satisfies the BCCE\nconditions.\nProof: We take the joint empirical distribution as in (9).\nDT(\u03b8, s, a 1, a2)=1\nTTX\nt=11{\u03b8t=\u03b8, st=s, at\n1=a1, at\n2=a2}.\nAssuming that each empirical frequency converges to some\nstationary probability distribution, which we will show to\nbe the case in later steps, we can write down the following\nexpression:\nDT(\u03b8, s, a 1, a2)T\u2192\u221e\u2212 \u2212 \u2212 \u2212 \u2192 P(\u03b8, s, a 1, a2).\nSince, the players\u2019 actions depend only on the received\nsignals, we can express the joint probability distribution as:\nP(\u03b8, s, a 1, a2) =P(\u03b8, s)P(a1, a2|s).\nSince the states {\u03b8t}are i.i.d. with distribution \u03c8(\u03b8), and\ngiven \u03b8t, the signal stis drawn according to \u03c0(s|\u03b8t), the\npairs (\u03b8t, st)are i.i.d.. Hence, by the Strong Law of Large\nNumbers (SLLN),\n1\nTTX\nt=11{\u03b8t=\u03b8, st=s}a.s.\u2212 \u2212 \u2192\u03c8(\u03b8)\u03c0(s|\u03b8)\nfor every (\u03b8, s)\u2208\u0398\u00d7S. To show the convergence of the\naction profiles, fix a signal s\u2208SwithP(s)>0and let Ts=\n{t\u2264T:st=s}.For each fixed action profile (a1, a2)\u2208\nA1\u00d7A2, define the empirical frequency of actions for t\u2208Ts\nas,Xt=1{a1\nt=a1, a2\nt=a2}.Then, the time-averaged\nfrequency over rounds when st=sis given by\nX|Ts|=1\n|Ts|X\nt\u2208TsXt.\nSince P(s)>0, we have |Ts| \u2192 \u221e asT\u2192 \u221e almost\nsurely. Moreover, it is well known by [21] that, when all\nplayers use no-regret algorithms, the empirical frequencies\nof players\u2019 actions almost surely converge to the CCE of\n\n--- Page 4 ---\nthe static game. Also, it can be observed that, due to the\nSLLN, the i.i.d. and stationary nature of the provided signals\nand state transition probabilities, as T\u2192 \u221e , for each\ngiven signal instance, an \u201cexpected\u201d or \u201cstatic\u201d game is\nformed, where the payoffs of the players are sampled from\nan i.i.d. distribution accordingly. Thus, no-regret algorithms\nguarantee the convergence in the new \u201cstatic\u201d game. Hence,\nwe obtain\nX|Ts|a.s.\u2212 \u2212 \u2192\u03c3(a1, a2|s),\nwhere \u03c3(\u00b7 |s)is the limiting distribution over A1\u00d7 A 2\nconditioned on the signal s, subject to the CCE set of the\nstationary game. Therefore, it follows that:\nDT(\u03b8, s, a 1, a2)a.s.\u2212 \u2212 \u2192D(\u03b8, s, a 1, a2)=\u03c8(\u03b8)\u03c0(s|\u03b8)\u03c3(a1, a2|s).\nSince the state, signal, and action spaces are finite, and the\npayoff functions ui(ai, a\u2212i, \u03b8)are bounded, we define, for\neach player i, the mapping given the signal sas:\nFi(DT(\u03b8,s,a i,a\u2212i) )=X\n\u03b8\u2208\u0398X\na\u2208ADT(\u03b8,s,a i,a\u2212i)ui(ai,a\u2212i,\u03b8).\nThus, it is easy to see that the function DT7\u2192Fi(DT)is\ncontinuous. Given that, aiis the action profile of player i,\nthe no-regret property guarantees that for every alternative\nfixed action a\u2032\ni\u2208 A i, the empirical distributions satisfy an\napproximate no-deviation inequality:\nFi(DT(\u03b8, s, a i, a\u2212i))\u2265Fi(DT(\u03b8, s, a\u2032\ni, a\u2212i))\u2212\u03f5T,\nwith \u03f5T\u21920asT\u2192 \u221e . By the Continuous Mapping\nTheorem [22], taking the limit as T\u2192 \u221e yields\nFi(D(\u03b8, s, a i, a\u2212i))\u2265Fi(D(\u03b8, s, a\u2032\ni, a\u2212i)),\nfor all ai, a\u2032\ni\u2208 Aiand for each player i. Then, summing both\nsides over the all possible signals, this expression becomes\nprecisely the BCCE condition:\nEDh\nui(ai, a\u2212i, \u03b8)i\n\u2265EDh\nui(a\u2032\ni, a\u2212i, \u03b8)i\n,\nwhich concludes the proof.\nHaving established convergence to the BCCE set under\nseparate no-regret algorithms conditioned on the given signal\ninstance, we now present the EXP3.P in Algorithm 1.\nThe EXP3.P algorithm operates over an adversarial multi-\narmed bandit setting with Karms, different actions, for\nTrounds. It maintains a probability vector pt\u2208\u2206Kand\ncumulative gain estimates {bGi,t}K\ni=1. Initially, p1is uniform\nover all arms and bGi,0= 0 for each i. At round t, the\nlearner samples an arm It\u223cptand observes a gain gIt,t\u2208\n[0,1]. To correct for partial feedback and to obtain high-\nprobability concentration, and limit the variance of gain\nestimates EXP3.P forms the importance-weighted estimate\nbgi,t, provided in Line 5 of Algorithm 1 with bias term \u03b2\u2208\n[0,1], and updates bGi,t=bGi,t\u22121+bgi,t. The next distribution\nis obtained by exponentiating the scaled cumulative estimates\nand mixing with a uniform exploration floor pt+1(i), given in\nLine 8 of Algorithm 1, where \u03b7 >0is the learning rate and\n\u03b3\u2208[0,1]controls minimum exploration. By intertwining the\nbias\u03b2and the exploration floor \u03b3, EXP3.P attains a regret\nbound of order O\u0000p\nTln(K/\u03b4)\u0001\nwith probability at least\n1\u2212\u03b4[12]. In the following theorem, we relate each instance\nof the regret algorithm given s, to the overall regret.Algorithm 1 EXP3.P\nRequire: Learning rate \u03b7 >0, parameters \u03b3, \u03b2, g i,t\u2208[0,1],\nnumber of arms K\n1:Initialize p1(i)\u21901\nK, and bGi,0\u21900for all i\u2208\n{1, . . . , K }\n2:fort= 1 tondo\n3: Sample arm It\u223cpt\n4: foreach arm i= 1 toKdo\n5: Compute estimated gain: bgi,t\u2190gi,t1{It=i}+\u03b2\npt(i)\n6: Update cumulative estimated gain: bGi,t\u2190bGi,t\u22121+bgi,t\n7: end for\n8: Update probabilities for next round:\npt+1(i)\u2190(1\u2212\u03b3)\u00b7exp(\u03b7bGi,t)PK\nk=1exp(\u03b7bGk,t)+\u03b3\nK\n9:end for\nTheorem 1: Define the overall-regret across the signal\ninstances by\nRovr(T) :=X\ns\u2208Smax\na\u2208AX\nt:st=s\u0010\nui,t(a, at\n\u2212i, \u03b8)\u2212ui,t(at\ni, at\n\u2212i, \u03b8)\u0011\n.\nAssume the following single-instance high-probability bound\nfor EXP3.P: there exists a constant C > 0such that for\nevery horizon T, number of actions |K|and confidence\n\u03b4\u2208(0,1), with probability of at least 1\u2212\u03b4,R(T)\u2264\n\u02dcR(T;\u03b4) := Cr\nK Tln\u0010\nK\n\u03b4\u0011\n.Then, with probability of at\nleast 1\u2212\u03b4,Rovr(T)\u2264\u221a\n2\u02dcR\u0010\nT;\u03b4\n2\u0011\n.\nProof: Letns:=\f\f{t\u2264T:st=s}\f\fdenote the number\nof rounds in which signal sappears, so that ns(1)+ns(2)=\nT. Apply the single-instance high-probability bound to this\nsubsequence with confidence parameter \u03b4s>0to obtain the\neventEs:=n\nR(ns)\u2264\u02dcR(ns;\u03b4s)o\n,which satisfies P(Es)\u2265\n1\u2212\u03b4s. Choose \u03b4s=\u03b4/2for both signals and invoke the union\nbound to get P(Es(1)\u2229 Es(2))\u22651\u2212P\ns\u2208S\u03b4s= 1\u2212\u03b4.\nOn the intersection event Es(1)\u2229 Es(2), we can sum the two\nbounds:\nRovr(T) =X\ns\u2208SR(ns)\u2264X\ns\u2208S\u02dcR(ns;\u03b4\n2).\nBy the assumed form of \u02dcR(\u00b7;\u00b7),\nX\ns\u2208S\u02dcR(ns;\u03b4\n2) = Cr\nKln\u0010\n2K\n\u03b4\u0011X\ns\u2208S\u221ans.\nFinally, apply Jensen\u2019s inequality for the concave map:\nX\ns\u2208S\u221ans\u2264s\n|S|X\ns\u2208Sns=\u221a\n2T.\nCombining the last two displays yields, on Es(1)\u2229 Es(2),\nRovr(T)\u2264Cr\nKln\u0010\n2K\n\u03b4\u0011\u221a\n2T=\u221a\n2\u02dcR\u0010\nT;\u03b4\n2\u0011\n,\nwhich establishes the stated bound with probability of at least\n1\u2212\u03b4.\nUpon reflecting on how to steer no-regret players toward\nspecific BCCE points, it becomes evident that this occurs\nwhen the BCCE set is a singleton containing only the\nmediator\u2019s target action profile. From this, we identify two\n\n--- Page 5 ---\npossible ways to achieve steering. The first case arises when\neach player has a strictly dominant action across all states.\nThe second approach involves designing mechanisms to\nensure that the BCCE set contains only a single point. Given\nthese observations, next we formalize the first case.\nLemma 2: Let each player i\u2208 I have a strictly dominant\naction a\u2217\ni\u2208 A iin every state \u03b8\u2208\u0398and for every signal\ns\u2208Sin the Bayesian game denoted by (I,A,\u0398, S, u).\nThen, the BCCE of the game is unique.\nProof: Since a\u2217\niis a strictly dominant action for player\ni, it satisfies, ui(a\u2217\ni, a\u2212i, \u03b8, s)> ui(ai, a\u2212i, \u03b8, s), for every\nai\u2208 A i\\ {a\u2217\ni}, for all a\u2212i\u2208 A\u2212i,\u03b8\u2208\u0398, and s\u2208S. In a\nBCCE, for each player i, for every signal s\u2208S, and for any\nalternative action a\u2032\ni\u2208Ai, the following condition holds:\nE\u03c8(\u03b8),\u03c0(s|\u03b8),\u03c3h\nui(ai,a\u2212i,\u03b8,s)i\n\u2265E\u03c8(\u03b8),\u03c0(s|\u03b8),\u03c3h\nui(a\u2032\ni,a\u2212i,\u03b8,s)i\n.\nBy the definition of a\u2217\ni, we have:\nE\u03c8(\u03b8),\u03c0(s|\u03b8),\u03c3h\nui(a\u2217\ni,a\u2212i,\u03b8,s)i\n>E\u03c8(\u03b8),\u03c0(s|\u03b8),\u03c3h\nui(ai,a\u2212i,\u03b8,s)i\nfor all ai\u2208Ai\\ {a\u2217\ni}. Now, suppose for contradiction\nthat there exists a BCCE \u03c3where player ichooses an\naction ai\u0338=a\u2217\niwith positive probability for some signal\ns. Then, consider a unilateral deviation by player ifrom\nthe recommended action aitoa\u2217\ni. Since a\u2217\niis a dominant\naction, ui(a\u2217\ni, a\u2212i, \u03b8, s)> ui(ai, a\u2212i, \u03b8, s), for all (a\u2212i, \u03b8, s).\nTaking expectations over \u03c8(\u03b8)and\u03c0(s|\u03b8), we get:\nE\u03c8(\u03b8),\u03c0(s|\u03b8)h\nui(a\u2217\ni, a\u2212i, \u03b8, s)i\n>E\u03c8(\u03b8),\u03c0(s|\u03b8)h\nui(ai, a\u2212i, \u03b8, s)i\n.\nThis contradicts the BCCE condition, which requires that\nthe expected utility from following the recommended action\nmust be at least as good as any deviation. Thus, the only\npossible distribution \u03c3that satisfies the BCCE condition is\nthe degenerate distribution where:\n\u03c3(a1, . . . , a n|s) = 1 ifai=a\u2217\nifor all i,\nand zero otherwise. Therefore, the BCCE is unique.\nRemark 1: Since Lemma 2 imposes no assumptions\non mechanisms, successful steering\u2014which is defined as\nachieving zero directness gap between the players\u2019 action\nprofiles and the target strategy point\u2014is always feasible.\nNext, we analyze the \u201cinvestment\u201d game by categorizing\nit into two regions depending on the sign of z+yB.\nProposition 1: Ifz+yB>0, then (I, I)is a strictly\ndominant strategy profile for both players, and therefore\n(I, I)becomes the unique BCCE.\nProof: Since yG>0, we have z+yG>0. Thus,\nifz+yB>0, regardless of the state or the other player\u2019s\naction, the payoff for choosing action Iis strictly positive,\nwhile choosing action Nyields 0. Thus, action Istrictly\ndominates action Nfor each player in each state. As a result,\ndue to Lemma 2, (I, I)becomes the unique BCCE.\nOn the other hand, when z+yB<0, a strictly dominating\nstrategy set does not exist, which necessitates the use of\ninformation and incentive design so that (I, I)target point\nremains the unique BCCE point. To derive this from the\ndefinition of the BCCE, consider the inequality for player ichoosing ai=Iover an alternative action a\u2032\ni=N:X\n\u03b8\u2208{G,B}X\ns\u2208{g,b}X\na\u2212i\u2208A\u2212iP(\u03b8, s, I, a \u2212i)ui(I, a\u2212i, \u03b8)\u2265\nX\n\u03b8\u2208{G,B}X\ns\u2208{g,b}X\na\u2212i\u2208A\u2212iP(\u03b8, s, N, a \u2212i)ui(N, a\u2212i, \u03b8),(10)\nwhich can be simplified as:X\n\u03b8\u2208{G,B}X\ns\u2208{g,b}X\na\u2212i\u2208A\u2212iP(\u03b8, s, I, a \u2212i)ui(I, a\u2212i, \u03b8)\u22650.(11)\nUsing this definition, we can analyze how (I, I)can be\nforced to be the unique BCCE point. Since the media-\ntor\u2019s objectives are twofold\u2014first, minimizing the amount\npaid to players, and second, reducing the directness gap to\nzero\u2014we analyze three cases as follows: a) Steering with\nonly information design, b) Steering with information design\naccompanied by sublinear payments, ensuring that the total\namount paid remains finite, and c) Steering with information\ndesign accompanied by linear payments. Thus, for these\ncases, we present the following two theorems demonstrating\nthe non-feasibility of the first two cases, and then provide\nanalysis of the last case.\nTheorem 2: There exists a game, within the provided\nsetting, without strictly dominant strategies in which no-\nregret players cannot be steered toward a mediator\u2019s target\nstrategy profile using only information design. Then, in such\ngames, successful steering is not possible without incentives.\nProof: First, we can rewrite (11) as:X\n\u03b8\u2208\u0398X\ns\u2208SX\na\u2212i\u2208A\u2212i\u03c8(\u03b8)\u03c0(s|\u03b8)\u03c3(I, a\u2212i|s)ui(I, a\u2212i, \u03b8)\u22650\nwhich gives:X\n\u03b8\u2208\u0398\u03c8(\u03b8)\u0010\n\u03c0(g|\u03b8)\u0000\n\u03c3(I, I|g)ui(I, I, \u03b8 )+\u03c3(I, N|g)ui(I, N, \u03b8 )\u0001\n+\u03c0(b|\u03b8)\u0000\n\u03c3(I, I|b)ui(I, I, \u03b8 )+\u03c3(I, N|b)ui(I, N, \u03b8 )\u0001\u0011\n\u22650.\nFor the sake of the counterexample, we consider:\n\u03c3(I|g) =\u03c3(I|b) =\u03c3(I, I|g) =\u03c3(I, I|b) = 1 .\nUnder this case, the BCCE no-deviation condition becomes:\n\u03c8(B)(z+yB) +\u03c8(G)(z+yG)\u22650.\nDepending on yB,yG, and z, the inequality cannot be forced\nin all cases. Thus, in such games, information design is not\nenough and we need to provide incentives as well, which\ncompletes the proof. Also, this counterexample directly ex-\ntends to the infeasibility of steering to a point in the BCE\nset with just information design, by assuming equal action\nprobabilities and showing that the BCE set cannot be forced\nto be a singleton due to the inequality from the definition of\nthe BCE set.\nTheorem 3: There exists a game, within the provided\nsetting, where information design, even when supported by\nsublinear payments, is insufficient to steer no-regret players\ntoward the mediator\u2019s target strategy profile. Consequently,\nin such games, successful steering is not possible without\nconstant average payments.\nProof: We denote the joint action probability of players\nas,\u03c3(I, I|s) = \u03c32(I|s) +\u03c1\u03c3(I|s)(1\u2212\u03c3(I|s)), where \u03c1\nrepresents the correlation between the two no-regret players,\n\n--- Page 6 ---\ndue to inference between players\u2019 learning processes. Note\nthat, introducing constant payments for the cases (I, I)or\nIdoes not result in sub-linear average payments. Also,\nvanishing payments alone are insufficient, as they cannot\nenforce BCCE on the players indefinitely. Therefore, the only\nviable option is to introduce constant payments for the (I, N)\nand(N, I)cases. Formally from (11), it can be seen that\nconstant payments in these cases cannot ensure the players\u2019\nconvergence to a specific BCCE point, as:X\n\u03b8\u2208\u0398X\ns\u2208SX\na\u2212i\u2208A\u2212i\u03c8(\u03b8)\u03c0(s|\u03b8)\u03c3(I, a\u2212i|s)ui(I, a\u2212i, \u03b8)\u22650\nwhich gives:X\n\u03b8\u2208\u0398\u03c8(\u03b8)\u0010\n\u03c0(g|\u03b8)\u0000\n\u03c3(I, I|g)ui(I, I, \u03b8 )+\u03c3(I, N|g)ui(I, N, \u03b8 )\u0001\n+\u03c0(b|\u03b8)\u0000\n\u03c3(I, I|b)ui(I, I, \u03b8 )+\u03c3(I, N|b)ui(I, N, \u03b8 )\u0001\u0011\n\u22650.\nFurthermore, this expression can be written as follows:\n\u03c8(B)\u0014\n\u03c0(g|B) [(\u03c3(I|g)\u2212\u03c3(I|b))(z+q)\n+ (\u03c3(I, I|g)\u2212\u03c3(I, I|b))(yB\u2212q)] +\u03c3(I|b)(z+q)\n+\u03c3(I, I|b)(yB\u2212q)\u0015\n+\u03c8(G)\u0014\n\u03c0(g|G) [(\u03c3(I|g)\n\u2212\u03c3(I|b))(z+q) + (\u03c3(I, I|g)\u2212\u03c3(I, I|b))(yG\u2212q)]\n+\u03c3(I|b)(z+q) +\u03c3(I, I|b)(yG\u2212q)\u0015\n\u22650\nwhere qis the constant payment introduced in (N, I)and\n(I, N). For the counterexample, we can assume the target\npoint as, \u03c3(I|g) =\u03c3(I|b), and \u03c3(I, I|g) =\u03c3(I, I|b).\nFor this specific case, we have that:\n\u03c8(B)\u0014\n(z+q) + [\u03c3(I|b) +\u03c1(1\u2212\u03c3(I|b)](yB\u2212q)\u0015\n+\u03c8(G)\u0014\n(z+q) + (yG\u2212q)[\u03c3(I|b) +\u03c1(1\u2212\u03c3(I|b))]\u0015\n\u22650\nwhich can be further manipulated into:\u0014\n\u03c8(B)yB+\u03c8(G)yG\u2212q\u0015\u0014\n\u03c3(I|b)(1\u2212\u03c1) +\u03c1\u0015\n\u2265 \u2212z\u2212q.\nIf(\u03c8(B)yB+\u03c8(G)yG)<0, we have:\n\u03c3(I|b)\u2264\u0014\u2212z\u2212q\n(\u03c8(B)yB+\u03c8(G)yG)\u2212q\u2212\u03c1\u00151\n(1\u2212\u03c1).\nThen, \u03c3(I|b)happens to be the unique point in the BCCE\nset, only when both sides of inequality meets at 0. Thus, such\nan incentive scheme cannot guarantee successful steering to\na specific point in the general case, concluding the proof.\nAs we have demonstrated that constant payments are\nnecessary for successful steering in all games, we introduce a\npayment scheme by deriving a lower bound on the payments\nrequired for each round. For this purpose, we define the\npayment bound Mas the total payments made, averaged\nover the sequences game has proceeded. Such a payment\nbound M, which ensures successful steering, can be derived\nusing the players\u2019 external regret. This follows from the fact\nthat regret minimization ensures that players will match the\nbest fixed hindsight action given the signal.\nTo identify the critical deviation, consider the scenario\nwhere a player deviates to action Nin the bad state \u03b8=B,\nand good state \u03b8=G. As mediator commits to an incentiveand information design scheme prior to observe the states\nof the world, we assume that it introduces constant payment\nacross the all states.\nFurthermore, toward bounding deviations, let D\u03b8denote\nthe total number of deviations for given state. By the defini-\ntion of the overall-regret, the cumulative deviation cost must\nsatisfy\nRovr(T) =DG(M+z+yG) +DB(M+z+yB).(12)\nThen, to bound total deviations using the bound we have\nderived for Rovr(T)we choose M > z +yB, which makes\nboth terms of the summand positive and (I, I)dominant\naction profile. Then, denoting \u03ba= min {M+z+yG, M+\nz+yB}, we bound the directness gap with probability of at\nleast 1\u2212\u03b4as:\n\u03b4(T) =D\nT\u2264Rovr(T)\n\u03baT\u2264\u221a\n2\u02dcR\u0010\nT;\u03b4\n2\u0011\n\u03baT(13)\nIV. S TACKELBERG OPTIMIZATION FRAMEWORK\nGiven the infeasibility of introducing stationary signal-\ning policy into the repeated game and the necessity of\nconstant payments, we incorporate information design as\na prior interaction between the mediator and the players.\nThis interaction is modeled as a Stackelberg game, where\nthe mediator is the leader, and the players are the followers\nwho can coordinate. More specifically, since both players are\nsymmetric and have identical preferences, they optimize over\nthe same utility function. Furthermore, we provide solutions\nfor the mediator\u2019s policy, where it prioritizes its steering\nobjective. The model is based on the stationary game before\nthe repeated game, with its payoff and action probability\nmatrices given in Section II.\nAt the upper level , the mediator selects the stationary\nsignaling policy parameters (\u03b1, \u03b2), which do not evolve over\niterations, to maximize its own objective function. At the\nlower level , given the mediator\u2019s choices (\u03b1, \u03b2), each player\nchooses its strategy parameters (\u03b1g, \u03b1b, \u03b3g, \u03b3b)to maximize\nits expected utility E[u]while satisfying the constraints im-\nposed by the Bayesian Correlated Equilibrium (BCE).3For\neach follower\u2019s optimization problem, the expected utility to\nbe maximized is given by:\nE[u] =Ag\u03b1g+Bg\u03b3g+Ab\u03b1b+Bb\u03b3b, (14)\nwhere the coefficients Ag,Bg,Ab, and Bbare defined based\non the upper-level variables (\u03b1, \u03b2)and other parameters:\nAg=\u03c8\u03b1z+ (1\u2212\u03c8)\u03b2z, (15)\nBg=\u03c8\u03b1y G+ (1\u2212\u03c8)\u03b2yB, (16)\nAb=\u03c8(1\u2212\u03b1)z+ (1\u2212\u03c8)(1\u2212\u03b2)z, (17)\nBb=\u03c8(1\u2212\u03b1)yG+(1\u2212\u03c8)(1\u2212\u03b2)yB, (18)\nwhere Aj\u2019s are always positive by definition. Furthermore,\nfor each type j\u2208 {g, b}, the decision variables must satisfy\nthe following constraints, arising from the BCE and action\nprobability constraints:\nAj\u03b1j+Bj\u03b3j\u22650,0\u2264\u03b3j\u2264\u03b1j\u22641,1\u22122\u03b1j+\u03b3j\u22650.(19)\n3Here, as the players have the symmetric utility functions, we denote the\nexpected utility function of any of the two players by E[u].\n\n--- Page 7 ---\n0.20.40.60.8 1 1.20.20.40.60.811.2\nA\nB C\u03b1j\u03b3j\nFig. 1. Feasible region defined by vertices A, B, and C along with their\nconvex combinations and constraints 0\u2264\u03b3j\u2264\u03b1jand1\u22122\u03b1j+\u03b3j\u22650,\nfor the cases of Bj>0and\u2212Bj< Aj.\nNote that the constraints for the good state ( j=g) and\nthe bad state ( j=b) are independent. Hence, the lower-\nlevel problem can be decomposed into two separate two-\ndimensional linear programs. Each subproblem can be solved\nindependently to obtain the optimal values of (\u03b1\u2217\nj, \u03b3\u2217\nj)for\nj\u2208 {g, b}. For each j\u2208 {g, b}, the optimization subproblem\nis defined as:\nmax\n{\u03b1j,\u03b3j}E[uj] =Aj\u03b1j+Bj\u03b3j, j\u2208 {g, b} (20)\ns.t.Aj\u03b1j+Bj\u03b3j\u22650 (21)\n0\u2264\u03b3j\u2264\u03b1j\u22641 (22)\n1\u22122\u03b1j+\u03b3j\u22650, (23)\nwhere E[u] =E[ug] +E[ub]. Each subproblem\u2019s feasible\nregion is a convex polygon in the (\u03b1j, \u03b3j)plane. Therefore,\nthe optimal solution lies at a convex combination of the ver-\ntices. The potential vertices are vertex A: (\u03b1j, \u03b3j) = (1 ,1),\nvertex B: (\u03b1j, \u03b3j) =\u00001\n2,0\u0001\n, and vertex C: (\u03b1j, \u03b3j) = (0 ,0),\nwhich has been plotted in Figure 1. The optimal solution\ncorresponds to the vertex with the highest value of E[uj].\nWe compare the expected utilities E[uj]evaluated at the\npoints A,B, and Cto determine which vertex is optimal\nunder different conditions. Vertex A is optimal if E[uj(A)]>\nE[uj(B)]andE[uj(A)]>E[uj(C)]. Substituting the ex-\npressions, we obtain Bj>\u2212Aj\n2for vertex A to be optimal.\nSimilarly, vertex B is optimal if E[uj(B)]>E[uj(A)]and\nE[uj(B)]>E[uj(C)]which imply that\n\u2212Aj\n2> B j. Finally, vertex C is optimal if E[uj(C)]>\nE[uj(A)]andE[uj(C)]>E[uj(B)].\nSubstituting the expressions, we obtain Aj+Bj<0and\nAj\n2<0which is not possible as Ajwas defined to be\npositive. Hence, vertex C cannot be optimal. To cover all\npossible scenarios, we analyze boundary conditions where\nequalities hold. A convex combination of vertex A and vertex\nB is optimal if E[uj(A)] =E[uj(B)]which implies that\nBj=\u2212Aj\n2. In this case, any point in between vertices A\nand B is optimal. For this special case, we assume that\nboth players prefer mediator preferred response which is\n(\u03b1\u2217\nj,\u03b3\u2217\nj)=(1 ,1).\nBy combining all the cases above, we derive the closed-\nform solutions for the pair (\u03b1j,\u03b3j)as follows:\n(\u03b1\u2217\nj, \u03b3\u2217\nj) =(\n(1,1) ifBj\u2265 \u2212Aj\n2,\u00001\n2,0\u0001\nifBj<\u2212Aj\n2.(24)Here, we note that the players will always take action\n(I, I)(corresponding to (\u03b1\u2217\nj, \u03b3\u2217\nj) = (1 ,1)in (24)) if Bj\u2265\n\u2212Aj\n2or take randomized actions (I, N)or(N, I)with\nprobability1\n2ifBj<\u2212Aj\n2. As AjandBjboth depends\non the mediator\u2019s policy (\u03b1, \u03b2), now we turn our attention\nto the mediator\u2019s optimization problem.\nIn this case, we define the expected utility function of the\nmediator as:\nE[um]=\u03b1g\u0010\n\u03c8\u03b1+(1\u2212\u03c8)\u03b2\u0011\n+\u03b1b\u0010\n\u03c8(1\u2212\u03b1)+(1\u2212\u03c8)(1\u2212\u03b2)\u0011\n.(25)\nThen, we solve the mediator\u2019s optimization problem by\nanalyzing the following four cases:\n1) Case 1: (\u03b1\u2217\ng,\u03b3\u2217\ng) =\u00001\n2,0\u0001\nand(\u03b1\u2217\nb, \u03b3\u2217\nb) =\u00001\n2,0\u0001\n. Then,\nthe utility of mediator becomes E[um]=0.5.\n2) Case 2: (\u03b1\u2217\ng, \u03b3\u2217\ng)=\u00001\n2,0\u0001\nand(\u03b1\u2217\nb, \u03b3\u2217\nb) = (1 ,1). Then,\nthe expected utility becomes:\nE[um]=0.5\u0010\n\u03c8\u03b1+(1\u2212\u03c8)\u03b2\u0011\n+\u0010\n\u03c8(1\u2212\u03b1)+(1\u2212\u03c8)(1\u2212\u03b2)\u0011\n3) Case 3: (\u03b1\u2217\ng, \u03b3\u2217\ng)=(1 ,1)and(\u03b1\u2217\nb, \u03b3\u2217\nb) =\u00001\n2,0\u0001\n, respec-\ntively. Then, the expected utility becomes:\nE[um]=\u0010\n\u03c8\u03b1+(1\u2212\u03c8)\u03b2\u0011\n+0.5\u0010\n\u03c8(1\u2212\u03b1)+(1\u2212\u03c8)(1\u2212\u03b2)\u0011\n4) Case 4: (\u03b1\u2217\ng,\u03b3\u2217\ng) = (1 ,1)and(\u03b1\u2217\nb, \u03b3\u2217\nb) = (1 ,1). Then,\nthe utility of the mediator becomes E[um] = 1 .\nThen, we can formally state the Stackelberg Equilibrium\n(SE) for the steering oriented mediator in the following\ntheorem.\nTheorem 4: The SE of the game is characterized by\nthe mediator\u2019s policy (\u03b1, \u03b2)and the players\u2019 strategies\n(\u03b1G, \u03b3G, \u03b1B, \u03b3B)for any 0\u2264\u03b7\u22641as follows:\n(\u03b1,\u03b2, \u03b1 G, \u03b3G, \u03b1B, \u03b3B) =\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\u001a\u0010\n1,\u03c8(yG+z\n2)\n(1\u2212\u03c8)(\u2212yB\u2212z\n2),1,1,0.5,0\u0011\n,\n\u0010\n0,1\u2212\u03c8(yG+z\n2)\n(1\u2212\u03c8)(\u2212yB\u2212z\n2),0.5,0,1,1\u0011\u001b\n, yB<\u2212\u03c8yG+z\n2\n1\u2212\u03c8,\n(\u03b7, \u03b7,1,1,1,1), y B\u2265\u2212\u03c8yG+z\n2\n1\u2212\u03c8,(26)\nfor any 0\u2264\u03b7\u22641.\nProof: It can be easily seen that the utility maximizer\ncase for the mediator is Case 4. For this case to be optimal,\nwe need Bj\u2265 \u2212Aj\n2forj\u2208 {g, b}which imply that:\n(1\u2212\u03c8)(\u2212yB\u2212z\n2)\n\u03c8(yG+z\n2)\u2264minn\u03b1\n\u03b2,1\u2212\u03b1\n1\u2212\u03b2o\n.\nChoosing \u03b1=\u03b2on the right side provides the loos-\nest bounds on yBwhich is given by yB\u2265 \u2212\u03c8yG+z\n2\n1\u2212\u03c8.\nTherefore, the SE happens to be (\u03b1, \u03b2, \u03b1 G, \u03b3G, \u03b1B, \u03b3B) =\n(\u03b7, \u03b7,1,1,1,1),for any 0\u2264\u03b7\u22641when yB\u2265 \u2212\u03c8yG+z\n2\n1\u2212\u03c8.\nWhen Case 4 is not feasible, the mediator\u2019s next best\noptions are Cases 2 and 3. For Case 2, due to mediator\u2019s\nobjective, the optimal signaling probabilities are obtained\nwith the lowest possible values for (\u03b1, \u03b2). Also, for this case\nto be optimal, we need Bg<\u2212Ag\n2andBb\u2265 \u2212Ab\n2which\nimply that:\n\u03b1\n\u03b2<(1\u2212\u03c8)(\u2212yB\u2212z\n2)\n\u03c8(yG+z\n2)\u22641\u2212\u03b1\n1\u2212\u03b2. (27)\nDue to the mediator\u2019s objective function E[um], the utility\nof the mediator is maximized when (\u03b1, \u03b2) =\u0000\n0,(1\u2212\n\n--- Page 8 ---\nAlgorithm 2 Soft Inducement-Aided Steering Algorithm\n1:Input: Initial game parameters z,yG,yB,\u03c8.\n2:Mediator observes utility functions and inputs. Computes\nthe Stackelberg equilibria. Then, commits to a signaling\nmechanism \u03c0(\u00b7|\u03b8)that leads to the best Stackelberg\nequilibrium for herself.\n3:Players observe the \u03c0(\u00b7|\u03b8), optimize their own utility\nfunctions, and arrive at the Stackelberg equilibrium.\n4:foreach time step t= 1,2, . . . T do\n5: Nature selects the state of the world \u03b8t, and mediator\ncommits to a \u03bdi.\n6: Mediator samples a public signal st\u223c\u03c0(\u00b7|\u03b8t).\n7: Each player iobserves stand selects action at\ni\u223c\n\u03c3i,t(\u00b7|st, ht).\n8: Players receive rewards based on joint actions and the\nrealized state, v(t)\ni, and update their decision strategies\n\u03c3i,t(\u00b7|st+1, ht+1)using no-regret learning algorithms.\n9:end for\n\u03c8(yG+z\n2)\n(1\u2212\u03c8)(\u2212yB\u2212z\n2))\u0001\n, with the condition of yB<\u2212\u03c8yG+z\n2\n1\u2212\u03c8. For\nCase 3, the optimal signaling probabilities are obtained with\nthe largest possible values of (\u03b1, \u03b2). Also, for Case 3 to be\noptimal, we need Bg\u2265 \u2212Ag\n2andBb<\u2212Ab\n2, implying that:\n1\u2212\u03b1\n1\u2212\u03b2<(1\u2212\u03c8)(\u2212yB\u2212z\n2)\n\u03c8(yG+z\n2)\u2264\u03b1\n\u03b2.\nIn Case 3, the utility of the mediator is maximized\nwhen (\u03b1, \u03b2) =\u0000\n1,\u03c8(yG+z\n2)\n(1\u2212\u03c8)(\u2212yB\u2212z\n2)\u0001\n, with the condition\nofyB<\u2212\u03c8yG+z\n2\n1\u2212\u03c8. Since the mediator\u2019s utility is\nthe same in both cases, we conclude that when\nyB<\u2212\u03c8yG+z\n2\n1\u2212\u03c8holds, there exist two Stackelberg\nEquilibria given by (\u03b1, \u03b2, \u03b1 G, \u03b3G, \u03b1B, \u03b3B)\u2208n\u0010\n0,1\u2212\n\u03c8(yG+z\n2)\n(1\u2212\u03c8)(\u2212yB\u2212z\n2),0.5,0,1,1\u0011\n,\u0010\n1,\u03c8(yG+z\n2)\n(1\u2212\u03c8)(\u2212yB\u2212z\n2),1,1,0.5,0\u0011o\n.\nFinally, for Case 1, we need Bj<\u2212Aj\n2forj\u2208 {g, b},\nwhich implies that:\nmaxn\u03b1\n\u03b2,1\u2212\u03b1\n1\u2212\u03b2o\n<(1\u2212\u03c8)(\u2212yB\u2212z\n2)\n\u03c8(yG+z\n2).\nThus, choosing the loosest bound, we observe that it coin-\ncides with the intervals of Cases 2 and 3. Since, the mediator\nis better off in these cases, no SE arise from Case 1.\nNow, since the Stackelberg Equilibria are known, the\nmediator can guide players toward the one that is best for\nher. Thus, the general structure of the information-aided\nsteering framework is outlined in Algorithm 2. Given this\nstructure, to bound the directness gap convergence rate, the\nBayesian games can be reduced to a special adversarial\nbandit problem. In this setting, we specifically focus on the\nEXP3.P algorithm as it provides an ex-post regret bound\nwith high probability rather than an expected regret bound,\nas ex-post regret is required for calculating the directness-\ngap convergence bound. Now, we first present the following\nlemma to streamline the analysis of high-probability regret\nbound for the EXP3.P class under non-uniform probability\ninitialization.Lemma 3: Fix\u03b2\u2208(0,1]. Then, for any \u03b4\u2208(0,1), with\nprobability of at least 1\u2212\u03b4,\nTX\nt=1gi,t\u2264TX\nt=1\u02c6gi,t+ln(\u03b4\u22121)\n\u03b2.\nProof: The proof of Lemma 3 follows directly from\n[12, Lemma 3.1].\nConsequently, we provide a high-probability regret bound\nfor the EXP3.P class under non-uniform probability initial-\nization.\nTheorem 5: Suppose that the parameters satisfy \u03b3\u22641\n2,\n(1 +\u03b2)K \u03b7\u2264\u03b3.Then, for any \u03b4\u2208(0,1), with probability\nof at least 1\u2212\u03b4, the regret RTof Exp3.P, Algorithm 1, with\ninitial weights wi,1=\u03c0iis bounded by\nRT\u2264\u03b2 TK +\u03b3 T+ (1 + \u03b2)\u03b7 K T \u2212ln(\u03c0\u2217)\n\u03b7+ln(K/\u03b4)\n\u03b2\n(28)\nChoosing appropriate \u03b2, \u03b3, \u03b7 one obtains the regret bound of:\nRT=\u02dcO\u0010\u221a\nTK\u0000\n4p\nln(1/\u03c0\u2217) + 2p\nln(K/\u03b4)\u0001\u0011\n,(29)\nwith probability of at least 1\u2212\u03b4, where \u03c0\u2217is the initial\nprobability of choosing the best hindsight action.\nProof: First, from Algorithm 1 it can be seen that,\npi,t\u2265\u03b3/K . Hence, we have\n\u03b7bgi,t\u2264\u03b71 +\u03b2\npi,t\u2264(1 +\u03b2)\u03b7K\n\u03b3\u22641, (30)\nwhich will be used with ex\u22641 +x+x2forx\u22641. Fix\nk\u2208[K]. Since Ei\u223cpt[bgi,t] =gIt,t+\u03b2K, summing over t\nyields\nTX\nt=1gk,t\u2212TX\nt=1gIt,t=\u03b2KT +TX\nt=1gk,t\u2212TX\nt=1Ei\u223cpt[bgi,t].\n(31)\nWrite pt= (1\u2212\u03b3)qt+\u03b3u, where uis uniform on [K].\nUsing the exact identity \u2212Eq[X] =1\n\u03b7lnEq[e\u03b7(X\u2212Eq[X])]\u2212\n1\n\u03b7lnEq[e\u03b7X],we have\n\u2212Ei\u223cpt[bgi,t]=\u2212(1\u2212\u03b3)Eqt[bgi,t]\u2212\u03b3Eu[bgi,t]\n=(1\u2212\u03b3)\"\n1\n\u03b7lnEqt[e\u03b7(bgi,t\u2212Eqt[bgi,t)]]\u22121\n\u03b7lnEqt[e\u03b7bgi,t]#\n\u2212\u03b3Eu[bgi,t]. (32)\nBy (30), x=\u03b7(bgi,t\u2212Eqtbgi,t)\u22641almost surely, and thus\nwe obtain\nlnEqt[e\u03b7(bgi,t\u2212Eqt[bgi,t])]\u2264Eqt\u0002\ne\u03b7(bgi,t\u2212Eqt[bgi,t])\u22121\u2212\u03b7(bgi,t\u2212Eqt[bgi,t])\u0003\n\u2264\u03b72Eqt\u0002\n(bgi,t\u2212Eqtbgi,t)2\u0003\n\u2264\u03b72Eqt\u0002\nbg2\ni,t\u0003\n.\nAlso, qi,t\u2264pi,t/(1\u2212\u03b3)andbgi,t\u2264(1 +\u03b2)/pi,timply\nEqt\u0002\nbg2\ni,t\u0003\n=KX\ni=1qi,tbg2\ni,t\u22641\n1\u2212\u03b3KX\ni=1pi,tbgi,t1+\u03b2\npi,t=1+\u03b2\n1\u2212\u03b3KX\ni=1bgi,t.\nDropping the nonpositive term \u2212\u03b3Eubgi,t\u22640from (32), we\nhave\n\u2212Ei\u223cpt[bgi,t]\u2264(1 +\u03b2)\u03b7KX\ni=1bgi,t\u22121\u2212\u03b3\n\u03b7lnEqt[e\u03b7bgi,t].\n(33)\n\n--- Page 9 ---\nSince wi,t+1=wi,te\u03b7bgi,tandqi,t=wi,t/P\njwj,t, we have\nEqt[e\u03b7bgi,t] =KX\ni=1qi,te\u03b7bgi,t=PK\ni=1wi,t+1PK\ni=1wi,t.\nSumming (33) over t= 1, . . . , T and usingPT\nt=1PK\ni=1bgi,t\n=PK\ni=1bGi,T\u2264Kmax jbGj,T, we get\n\u2212TX\nt=1Ei\u223cpt[bgi,t]\u2264(1+\u03b2)\u03b7Kmax\njbGj,T\u22121\u2212\u03b3\n\u03b7TX\nt=1lnP\niwi,t+1P\niwi,t\n=(1+ \u03b2)\u03b7Kmax\njbGj,T\u22121\u2212\u03b3\n\u03b7lnP\niwi,T+1P\niwi,1\n=(1+ \u03b2)\u03b7Kmax\njbGj,T\u22121\u2212\u03b3\n\u03b7lnhKX\ni=1\u03c0ie\u03b7bGi,Ti\n(34)\nbecauseP\niwi,1=P\ni\u03c0i=1andwi,T+1=\u03c0ie\u03b7bGi,T. Leti\u22c6\u2208\narg max iPT\nt=1gi,t; thenP\ni\u03c0ie\u03b7bGi,T\u2265\u03c0\u22c6e\u03b7bGi\u22c6,T. Thus,\n\u2212TX\nt=1Ei\u223cpt[bgi,t]\u2264(1+\u03b2)\u03b7Kmax\njbGj,T\u2212(1\u2212\u03b3)bGi\u22c6,T\n\u22121\u2212\u03b3\n\u03b7ln(\u03c0\u22c6) (35)\nUsing Lemma 3, we have max jbGj,T\u2265max jPT\nt=1gj,t\u2212\nln(K/\u03b4)\n\u03b2=PT\nt=1gi\u22c6,t\u2212ln(K/\u03b4)\n\u03b2. Plugging this lower bound\ninto (35) gives\n\u2212TX\nt=1Ei\u223cpt[bgi,t]\u2264\u2212h\n1\u2212\u03b3\u2212(1+\u03b2)\u03b7KiTX\nt=1gi\u22c6,t\u22121\u2212\u03b3\n\u03b7ln(\u03c0\u22c6)\n+h\n1\u2212\u03b3\u2212(1 +\u03b2)\u03b7Kiln(K/\u03b4)\n\u03b2.\nCombining with (31) for k=i\u22c6yields\nRT\u2264\u03b2KT +\u0010\n\u03b3+ (1 + \u03b2)\u03b7K\u0011TX\nt=1gi\u22c6,t\u22121\u2212\u03b3\n\u03b7ln(\u03c0\u22c6)\n+h\n1\u2212\u03b3\u2212(1 +\u03b2)\u03b7Kiln(K/\u03b4)\n\u03b2.\nUsingPT\nt=1gi\u22c6,t\u2264Tand1\u2212\u03b3\u2212(1+\u03b2)\u03b7K\u22641, we obtain\nRT\u2264\u03b2KT +\u03b3T+(1+ \u03b2)\u03b7KT\u22121\u2212\u03b3\n\u03b7ln(\u03c0\u22c6)+ln(K/\u03b4)\n\u03b2.\nFinally, since 1\u2212\u03b3\u22641andln(\u03c0\u22c6)\u22640, loosening\n\u22121\u2212\u03b3\n\u03b7ln(\u03c0\u22c6)\u2264 \u22121\n\u03b7ln(\u03c0\u22c6)gives (28). Finally, we set \u03b2=q\nln(K/\u03b4)\nKT, \u03b7=q\nln(1/\u03c0\u22c6)\nKT, \u03b3= (1 + \u03b2)K \u03b7. Plugging into\n(28) and grouping terms gives\nRT\u2264p\nKTln(K/\u03b4)|{z }\n\u03b2KT+(1+ \u03b2)p\nKTln(1/\u03c0\u22c6)| {z }\n\u03b3T+p\nKTln(1/\u03c0\u22c6)|{z }\n\u2212ln(\u03c0\u22c6)/\u03b7\n+ (1 + \u03b2)p\nKTln(1/\u03c0\u22c6)| {z }\n(1+\u03b2)\u03b7KT+p\nKTln(K/\u03b4)|{z }\nln(K/\u03b4)/\u03b2,\nwhich simplifies to the announced \u02dcO\u0000\u221a\nKT(4p\nln(1/\u03c0\u22c6) +\n2p\nln(K/\u03b4))\u0001\n.\nThen, the result for directness gap convergence rate can\nbe stated as follows:\nTheorem 6: Let R(T) denote the regret of\nregular no-regret learners and R\u2217(T) denote\nthe regret of the SE-initiated players, for a\nFig. 2. Trajectories for the yB\u2265 \u2212\u03c8yG+z\n2\n1\u2212\u03c8case. With, \u03c8= 0.7,\n\u03b1= 0.7,\u03b2= 0.7,z= 0.2,yG= 1,yB=\u22120.05, penalty parameter\nM= 0.24, the learning rate \u03b7= 0.05for the EXP3.P algorithm, .\ngiven signal type. Then, we have R\u2217(T) =\n\u02dcO\u0010\u221a\nTK\u0010\n4p\nln(1/\u03c0\u2217) + 2p\nln(K/\u03b4)\u0011\u0011\n,which improves\nupon R(T) = \u02dcO\u0010\u221a\nTK\u0010\n4p\nln(K) + 2p\nln(K/\u03b4)\u0011\u0011\n.\nThis result extends to the directness gap convergence rate as\n\u03b4\u2217(T)=R\u2217\novr(T)\n\u03baT=\u02dcO \n2p\n2Kln(2K/\u03b4)+4\u221a\u03b3K\u221a\nT\u03ba!\nwhich provides a tighter bound compared to\n\u03b4(T)=Rovr(T)\n\u03baT=\u02dcO \n2p\n2Kln(2K/\u03b4)+4\u221a\n2KlnK\u221a\nT\u03ba!\nHere, \u03b3 <1denotes the fraction of timesteps\u2014multiplied\nbyln\u00001\n\u03c0\u2217\u0001\n\u2014in which the received signal did notlead the\nplayers to \u03b1s= 1 in the Stackelberg equilibrium.\nProof: First part of the theorem trivially follows from\nTheorem 5 and \u03c0\u2217\u22651\n2=1\nK. To derive the upper bound\nforR\u2217\novr, we consider the worst SE arising from Case 3. Let\nT1denote the good signal rounds, T2denote the bad signal\nrounds, and T=T1+T2. Then, having K= 2 =1\n\u03c0\u2217\nT2:\n(2\u221a\nKp\nln(2K/\u03b4)(p\nT1+p\nT2) +4p\nT2K\u221a\nln 2)\n\u2264(2p\n2(T1+T2)Kp\nln(2K/\u03b4) + 4p\nT2K\u221a\nln 2) = f(T)\n<(2\u221a\n2TKp\nln(2K/\u03b4) + 4\u221a\n2TK\u221a\nln 2) = g(T)\nwhere the \u02dcO(f(T)) = R\u2217\novr(T), and \u02dcO(g(T)) = Rovr(T).\nFinally, the second part follows from the above inequalities,\nalong with (13), which concludes the proof.\nV. N UMERICAL EXPERIMENTS\nIn the experiments, we compare the directness gap of\nregular players with that of SE-initiated players. We analyze\ntwo different cases. In the first case, yB\u2265 \u2212\u03c8yG+z\n2\n1\u2212\u03c8,the\nSE is initiated as (\u03b1, \u03b2, \u03b1 G, \u03b3G, \u03b1B, \u03b3B) = ( \u03b7, \u03b7,1,1,1,1),\nand the convergence results are depicted in Figure 2. In\nthe second case, yB<\u2212\u03c8yG+z\n2\n1\u2212\u03c8,the SE is initiated as\n(\u03b1, \u03b2, \u03b1 G, \u03b3G, \u03b1B, \u03b3B) = (0 ,0,0.5,0,1,1),for the chosen\nparameter values, with the results shown in Figure 3.\nEach player maintains two separate instances of the\nEXP3.P algorithm, one for each signal instance. As described\nin the theoretical analysis of the previous sections, in our\nexperiments, after the mediator commits to a stationary\nincentive and information-signaling scheme following the\n\n--- Page 10 ---\ninitial SE, it maintains this scheme throughout all iterations\nwithout updating it.\nIn Figures 2 and 3, we report on the evolution of the\ndirectness gap, where solid lines denote the mean of 50\nindependent runs and the shaded area represents one stan-\ndard deviation around the mean. To preserve exploration,\nprobabilities on the order of 10\u22122were used in place of\nzero probabilities that would otherwise be assigned by the\nSEs at initialization.\nFinally, we observe in the numerical experiments that our\nsoft-inducement-assisted prior Stackelberg game framework\noutperforms the randomly initialized no-regret players, as\nalso proven theoretically.\nVI. C ONCLUSION\nIn this work, we have addressed the problem of steering\nno-regret players with incentive and information design. We\nhave shown that successful steering is not feasible through\ninformation design alone, or accompanied with sublinear\npayments. First, we derived a lower bound on the required\naverage payments. Then, we proposed an information design-\nbased initiation of players for the repeated normal-form\ngame. Next, we established improved directness gap conver-\ngence rate for the proposed framework. Finally, we supported\nthese improved bounds with numerical experiments. Future\nwork will focus on improving asymptotic regret bounds\nthrough carefully crafted information design in games with\nside information and no-regret players.\nREFERENCES\n[1] T. Bas \u00b8ar, \u201cInducement of desired behavior via soft policies,\u201d Interna-\ntional Game Theory Review , vol. 26, no. 02, p. 2440002, 2024.\n[2] Y . Babichenko, I. Talgam-Cohen, H. Xu, and K. Zabarnyi, \u201cMulti-\nchannel Bayesian persuasion,\u201d arXiv preprint arXiv:2111.09789 , 2021.\n[3] D. McCloskey and A. Klamer, \u201cOne quarter of GDP is persuasion,\u201d\nThe American Economic Review , vol. 85, no. 2, pp. 191\u2013195, 1995.\n[4] G. Egorov and K. Sonin, \u201cPersuasion on networks,\u201d National Bureau\nof Economic Research, Tech. Rep., 2020.\n[5] J. P. Johnson and D. P. Myatt, \u201cOn the simple economics of adver-\ntising, marketing, and product design,\u201d American Economic Review ,\nvol. 96, no. 3, pp. 756\u2013784, 2006.\n[6] T. T. Ke, S. Lin, and M. Y . Lu, Information Design of Online\nPlatforms . SSRN, 2022.\n[7] I. Goldstein and C. Huang, \u201cBayesian persuasion in coordination\ngames,\u201d American Economic Review , vol. 106, no. 5, pp. 592\u2013596,\n2016.\n[8] I. Goldstein and Y . Leitner, \u201cStress tests and information disclosure,\u201d\nJournal of Economic Theory , vol. 177, pp. 34\u201369, 2018.\n[9] C. Yzermans, \u201cA systematic review of rapid needs assessments and\ntheir usefulness for disaster decision making: methods, strengths and\nweaknesses and value for disaster relief policy,\u201d International Journal\nof Disaster Risk Reduction , vol. 71, p. 102807, 2022.\n[10] A. S. Blinder and M. Zandi, \u201cThe financial crisis: Lessons for the next\none,\u201d Center on Budget and Policy Priorities: Policy Futures , 2015.\n[11] J. C. Harsanyi, \u201cGames with incomplete information played by\n\u201cBayesian\u201d Players, I\u2013III Part I. the basic model,\u201d Management\nScience , vol. 14, no. 3, 1967.\n[12] S. Bubeck, N. Cesa-Bianchi et al. , \u201cRegret analysis of stochastic and\nnonstochastic multi-armed bandit problems,\u201d Foundations and Trends\nin Machine Learning , vol. 5, no. 1, pp. 1\u2013122, 2012.\n[13] H. von Stackelberg, Marktform und Gleichgewicht . J. Springer, 1934.\n[14] D. Mguni, J. Jennings, S. V . Macua, E. Sison, S. Ceppi, and E. M.\nDe Cote, \u201cCoordinating the crowd: Inducing desirable equilibria in\nnon-cooperative systems,\u201d arXiv preprint arXiv:1901.10923 , 2019.\nFig. 3. Trajectories for the yB<\u2212\u03c8yG+z\n2\n1\u2212\u03c8case. With, \u03c8= 0.7,\n\u03b1= 0,\u03b2= 0,z= 0.2,yG= 0.1, and yB=\u22120.56, penalty parameter\nM= 0.60, the learning rate \u03b7= 0.05for the EXP3.P algorithm.\n[15] B. Liu, J. Li, Z. Yang, H.-T. Wai, M. Hong, Y . Nie, and Z. Wang,\n\u201cInducing equilibria via incentives: Simultaneous design-and-play en-\nsures global convergence,\u201d Advances in Neural Information Processing\nSystems , vol. 35, pp. 29 001\u201329 013, 2022.\n[16] B. H. Zhang, G. Farina, I. Anagnostides, F. Cacciamani, S. M.\nMcAleer, A. A. Haupt, A. Celli, N. Gatti, V . Conitzer, and T. Sand-\nholm, \u201cSteering no-regret learners to a desired equilibrium,\u201d arXiv\npreprint arXiv:2306.05221 , 2023.\n[17] V . P. Crawford and J. Sobel, \u201cStrategic information transmission,\u201d\nEconometrica: Journal of the Econometric Society , pp. 1431\u20131451,\n1982.\n[18] E. Kamenica and M. Gentzkow, \u201cBayesian persuasion,\u201d American\nEconomic Review , vol. 101, no. 6, pp. 2590\u20132615, 2011.\n[19] D. Bergemann and S. Morris, \u201cBayes correlated equilibrium and the\ncomparison of information structures in games,\u201d Theoretical Eco-\nnomics , vol. 11, no. 2, pp. 487\u2013522, 2016.\n[20] J. Hartline, V . Syrgkanis, and E. Tardos, \u201cNo-regret learning in\nBayesian games,\u201d Advances in Neural Information Processing Systems ,\n2015.\n[21] N. Cesa-Bianchi and G. Lugosi, Prediction, Learning, and Games .\nCambridge University Press, 2006.\n[22] J. Shao, Mathematical Statistics . Springer New York, NY , 2003.",
  "project_dir": "artifacts/projects/SoftInducementSteeringFramework",
  "communication_dir": "artifacts/projects/SoftInducementSteeringFramework/.agent_comm",
  "assigned_at": "2025-09-03T20:46:37.201235",
  "status": "assigned"
}